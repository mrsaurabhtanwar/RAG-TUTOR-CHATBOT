# ðŸš€ Render Deployment Guide - RAG Tutor Chatbot

## Quick Start Deployment

### Prerequisites
- [ ] GitHub account with repository access
- [ ] Render account (free tier available)
- [ ] OpenRouter API key (minimum required)

### Step 1: Prepare Repository
Your repository is already configured! Essential files:
- âœ… `fastapi_app.py` - Main application
- âœ… `requirements.txt` - Dependencies  
- âœ… `build.sh` - Build script
- âœ… `render.yaml` - Render configuration
- âœ… `.env.example` - Environment template

### Step 2: Deploy to Render

#### Option A: Using Render Dashboard (Recommended)
1. **Connect Repository**
   - Go to [Render Dashboard](https://dashboard.render.com/)
   - Click "New +" â†’ "Web Service"
   - Connect your GitHub repository: `mrsaurabhtanwar/RAG-TUTOR-CHATBOT`

2. **Configure Service**
   ```
   Name: rag-tutor-chatbot
   Environment: Python 3
   Build Command: chmod +x build.sh && ./build.sh
   Start Command: uvicorn fastapi_app:app --host 0.0.0.0 --port $PORT
   ```

3. **Set Environment Variables**
   ```
   OPENROUTER_API_KEY=your_openrouter_key_here
   GROQ_API_KEY=your_groq_key_here (optional)
   HUGGINGFACE_API_KEY=your_hf_key_here (optional)
   ```

#### Option B: Using Blueprint (render.yaml)
1. **Deploy with Blueprint**
   - Fork/clone repository
   - In Render Dashboard: "New +" â†’ "Blueprint"
   - Connect repository and deploy

### Step 3: Get API Keys

#### Required (Minimum)
- **OpenRouter API Key**: [Get it here](https://openrouter.ai/keys)
  - Provides access to multiple AI models
  - Free tier available with rate limits

#### Optional (Enhanced Features)
- **Groq API Key**: [Get it here](https://console.groq.com/keys)
  - Fast inference for Llama models
  - Free tier: 6,000 tokens/minute
  
- **HuggingFace API Key**: [Get it here](https://huggingface.co/settings/tokens)
  - Access to HuggingFace models
  - Free tier available

### Step 4: Verify Deployment

1. **Health Check**
   ```bash
   curl https://your-app.onrender.com/health
   ```

2. **API Test**
   ```bash
   curl -X POST "https://your-app.onrender.com/ask" \
        -H "Content-Type: application/json" \
        -d '{"question": "Hello, can you help me learn Python?"}'
   ```

3. **Interactive Docs**
   - Visit: `https://your-app.onrender.com/docs`

## ðŸ”§ Configuration Details

### Environment Variables Explained
```bash
# Required
OPENROUTER_API_KEY=sk-or-...     # Primary AI provider

# Optional (Fallbacks)
GROQ_API_KEY=gsk_...             # Fast Llama models
HUGGINGFACE_API_KEY=hf_...       # HuggingFace models

# Optional (Resource Search)
RAPIDAPI_KEY=your_key            # YouTube search
GOOGLE_API_KEY=your_key          # Google Custom Search
GOOGLE_CX=your_cx_id             # Google Search Engine ID

# Auto-generated by Render
PORT=10000                       # Render assigns this automatically
```

### Free Tier Limits
- **Render Free Tier**:
  - 750 hours/month (about 1 month continuous)
  - Sleeps after 15min inactivity
  - 512MB RAM, 0.1 CPU
  - Custom domain not included

- **API Rate Limits**:
  - OpenRouter: Varies by model
  - Groq: 6,000 tokens/minute (free)
  - HuggingFace: 1,000 requests/hour (free)

## ðŸš¨ Troubleshooting

### Common Issues

#### Build Failures
```bash
# Check build logs in Render dashboard
# Common fixes:
# 1. Ensure requirements.txt has all dependencies
# 2. Check Python version compatibility
# 3. Verify build.sh permissions
```

#### App Won't Start
```bash
# Check if PORT environment variable is used correctly
# Verify start command: uvicorn fastapi_app:app --host 0.0.0.0 --port $PORT
```

#### API Key Errors
```bash
# Verify environment variables are set in Render dashboard
# Check API key format and permissions
# Ensure at least OPENROUTER_API_KEY is set
```

#### Performance Issues
```bash
# Free tier limitations:
# - Cold start delays (15-30 seconds)
# - Limited memory (512MB)
# - CPU throttling

# Solutions:
# - Keep app warm with uptime monitoring
# - Optimize memory usage
# - Use efficient AI models
```

### Health Check Endpoint
```python
# Available at /health
{
    "status": "healthy",
    "timestamp": "2025-08-18T10:30:00Z",
    "version": "1.0.0",
    "environment": "production"
}
```

## ðŸ“Š Monitoring

### Built-in Metrics
- Request count and latency
- Error rates by endpoint
- API provider usage stats
- System resource usage

### Logs Access
- Render Dashboard â†’ Service â†’ Logs tab
- Real-time log streaming available
- Structured JSON logging implemented

## ðŸ”„ Updates & Maintenance

### Automatic Deployments
- Connected to GitHub repository
- Auto-deploys on `main` branch pushes
- Manual deploy option available

### Scaling Options
- **Starter Plan** ($7/month): No sleep, more resources
- **Standard Plan** ($25/month): Autoscaling, more CPU/RAM
- **Pro Plan** ($85/month): Priority support, advanced features

## ðŸŽ¯ Production Checklist

- [x] Essential files configured
- [x] Build script optimized
- [x] Start command configured
- [x] Health check endpoint available
- [x] Environment variables template ready
- [ ] Repository pushed to GitHub
- [ ] Render service created
- [ ] Environment variables set
- [ ] Deployment tested
- [ ] API endpoints verified

## ðŸ“ž Support

- **Render Support**: [Render Docs](https://render.com/docs)
- **FastAPI Docs**: [FastAPI Guide](https://fastapi.tiangolo.com/)
- **Repository Issues**: GitHub Issues tab

---

**Ready to deploy!** Your application is fully configured for Render's free tier. Just push to GitHub and follow the deployment steps above. ðŸš€
